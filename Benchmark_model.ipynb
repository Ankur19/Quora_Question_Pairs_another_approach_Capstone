{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "from __future__ import division \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import pickle\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Merge, TimeDistributed, Lambda, Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import backend as K\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.preprocessing import sequence, text\n",
    "\n",
    "from keras.layers import Input, Bidirectional, LSTM, dot, Flatten, Dense, Reshape, add, Dropout, BatchNormalization, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use pickle to import our pre=processed data \n",
    "import pickle\n",
    "with open('question_pair.pickle', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tk = text.Tokenizer(num_words=200000)\n",
    "# we use keras Tokenizer to tokenizer the data. \n",
    "# we will only consider top 200000 words that occur in the dataset\n",
    "\n",
    "max_len = 30\n",
    "# the maximum length of each sequence\n",
    "\n",
    "tk.fit_on_texts(list(df.question1.values.astype(str)) + list(df.question2.values.astype(str)))\n",
    "\n",
    "#we now convert the text to numerical data\n",
    "x1 = tk.texts_to_sequences(df.question1.values.astype(str))\n",
    "x2 = tk.texts_to_sequences(df.question2.values.astype(str))\n",
    "\n",
    "# we pad the sequences so that all questions are of the same length(30)\n",
    "x1 = sequence.pad_sequences(x1, maxlen=max_len)\n",
    "x2 = sequence.pad_sequences(x2, maxlen=max_len)\n",
    "\n",
    "#the word_index contains the words in our dataset mapped to numbers\n",
    "word_index = tk.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y is our target variable\n",
    "y = df.is_duplicate.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we split our data to train and test sets\n",
    "\n",
    "x1_train, x1_test, x2_train, x2_test, y_train, y_test = train_test_split(x1,x2,y, test_size = 0.1,random_state=2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we import our pretrained weights for the words in our dataset. \n",
    "#This file was created during preprocessing\n",
    "embedding_matrix = np.loadtxt('embeddings.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_4 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 30, 300)       26510400    input_3[0][0]                    \n",
      "                                                                   input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 50)            70200       embedding_2[0][0]                \n",
      "                                                                   embedding_2[1][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (None, 1)             0           lstm_1[0][0]                     \n",
      "                                                                   lstm_1[1][0]                     \n",
      "====================================================================================================\n",
      "Total params: 26,580,600\n",
      "Trainable params: 70,200\n",
      "Non-trainable params: 26,510,400\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model variables\n",
    "n_hidden = 50\n",
    "gradient_clipping_norm = 1.25\n",
    "batch_size = 500\n",
    "n_epoch = 100\n",
    "\n",
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
    "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
    "\n",
    "ques1 = Input(shape=(30,))\n",
    "ques2 = Input(shape=(30,))\n",
    "embedding_layer = Embedding(len(word_index) + 1, \n",
    "                 300, \n",
    "                 weights=[embedding_matrix], \n",
    "                 input_length=30, \n",
    "                 trainable=False)\n",
    "\n",
    "# Embedded version of the inputs\n",
    "encoded_left = embedding_layer(ques1)\n",
    "encoded_right = embedding_layer(ques2)\n",
    "\n",
    "# Since this is a siamese network, both sides share the same LSTM\n",
    "shared_lstm = LSTM(n_hidden)\n",
    "\n",
    "left_output = shared_lstm(encoded_left)\n",
    "right_output = shared_lstm(encoded_right)\n",
    "\n",
    "# Calculates the distance as defined by the MaLSTM model\n",
    "malstm_distance = Lambda(lambda x: exponent_neg_manhattan_distance(x[0], x[1]), output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
    "\n",
    "# Pack it all up into a model\n",
    "malstm = Model([ques1, ques2], [malstm_distance])\n",
    "\n",
    "# Adadelta optimizer, with gradient clipping by norm\n",
    "optimizer = Adadelta(clipnorm=gradient_clipping_norm)\n",
    "\n",
    "checkpoint = ModelCheckpoint('benchmark.h5', monitor='val_acc', save_best_only=True, verbose=2)\n",
    "\n",
    "estop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=20, verbose=0, mode='auto')\n",
    "\n",
    "malstm.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n",
    "malstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 327474 samples, validate on 36387 samples\n",
      "Epoch 1/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2080 - acc: 0.7020Epoch 00000: val_acc improved from -inf to 0.73356, saving model to benchmark.h5\n",
      "327474/327474 [==============================] - 81s - loss: 0.2079 - acc: 0.7020 - val_loss: 0.1779 - val_acc: 0.7336\n",
      "Epoch 2/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1697 - acc: 0.7483Epoch 00001: val_acc improved from 0.73356 to 0.75535, saving model to benchmark.h5\n",
      "327474/327474 [==============================] - 81s - loss: 0.1698 - acc: 0.7482 - val_loss: 0.1662 - val_acc: 0.7554\n",
      "Epoch 3/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1609 - acc: 0.7654Epoch 00002: val_acc improved from 0.75535 to 0.76662, saving model to benchmark.h5\n",
      "327474/327474 [==============================] - 80s - loss: 0.1609 - acc: 0.7654 - val_loss: 0.1607 - val_acc: 0.7666\n",
      "Epoch 4/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1557 - acc: 0.7756Epoch 00003: val_acc improved from 0.76662 to 0.77360, saving model to benchmark.h5\n",
      "327474/327474 [==============================] - 87s - loss: 0.1557 - acc: 0.7755 - val_loss: 0.1567 - val_acc: 0.7736\n",
      "Epoch 5/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1519 - acc: 0.7826Epoch 00004: val_acc improved from 0.77360 to 0.77827, saving model to benchmark.h5\n",
      "327474/327474 [==============================] - 89s - loss: 0.1519 - acc: 0.7826 - val_loss: 0.1541 - val_acc: 0.7783\n",
      "Epoch 6/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1491 - acc: 0.7881Epoch 00005: val_acc improved from 0.77827 to 0.78322, saving model to benchmark.h5\n",
      "327474/327474 [==============================] - 92s - loss: 0.1491 - acc: 0.7881 - val_loss: 0.1519 - val_acc: 0.7832\n",
      "Epoch 7/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1468 - acc: 0.7926Epoch 00006: val_acc improved from 0.78322 to 0.78561, saving model to benchmark.h5\n",
      "327474/327474 [==============================] - 84s - loss: 0.1468 - acc: 0.7926 - val_loss: 0.1502 - val_acc: 0.7856\n",
      "Epoch 8/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1449 - acc: 0.7961Epoch 00007: val_acc improved from 0.78561 to 0.79014, saving model to benchmark.h5\n",
      "327474/327474 [==============================] - 92s - loss: 0.1449 - acc: 0.7961 - val_loss: 0.1486 - val_acc: 0.7901\n",
      "Epoch 9/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1433 - acc: 0.7993Epoch 00008: val_acc improved from 0.79014 to 0.79284, saving model to benchmark.h5\n",
      "327474/327474 [==============================] - 94s - loss: 0.1433 - acc: 0.7993 - val_loss: 0.1475 - val_acc: 0.7928\n",
      "Epoch 10/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1419 - acc: 0.8021Epoch 00009: val_acc did not improve\n",
      "327474/327474 [==============================] - 92s - loss: 0.1418 - acc: 0.8021 - val_loss: 0.1464 - val_acc: 0.7926\n",
      "Epoch 11/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1406 - acc: 0.8040Epoch 00010: val_acc improved from 0.79284 to 0.79504, saving model to benchmark.h5\n",
      "327474/327474 [==============================] - 91s - loss: 0.1406 - acc: 0.8040 - val_loss: 0.1454 - val_acc: 0.7950\n",
      "Epoch 12/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1395 - acc: 0.8058Epoch 00011: val_acc improved from 0.79504 to 0.79649, saving model to benchmark.h5\n",
      "327474/327474 [==============================] - 80s - loss: 0.1395 - acc: 0.8058 - val_loss: 0.1447 - val_acc: 0.7965\n",
      "Epoch 13/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1386 - acc: 0.8077Epoch 00012: val_acc improved from 0.79649 to 0.79773, saving model to benchmark.h5\n",
      "327474/327474 [==============================] - 90s - loss: 0.1386 - acc: 0.8077 - val_loss: 0.1437 - val_acc: 0.7977\n",
      "Epoch 14/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1377 - acc: 0.8090Epoch 00013: val_acc improved from 0.79773 to 0.79850, saving model to benchmark.h5\n",
      "327474/327474 [==============================] - 93s - loss: 0.1377 - acc: 0.8090 - val_loss: 0.1431 - val_acc: 0.7985\n",
      "Epoch 15/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1368 - acc: 0.8104Epoch 00014: val_acc improved from 0.79850 to 0.80125, saving model to benchmark.h5\n",
      "327474/327474 [==============================] - 86s - loss: 0.1368 - acc: 0.8105 - val_loss: 0.1423 - val_acc: 0.8012\n",
      "Epoch 16/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.8114Epoch 00015: val_acc improved from 0.80125 to 0.80298, saving model to benchmark.h5\n",
      "327474/327474 [==============================] - 92s - loss: 0.1361 - acc: 0.8114 - val_loss: 0.1421 - val_acc: 0.8030\n",
      "Epoch 17/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.8128Epoch 00016: val_acc improved from 0.80298 to 0.80312, saving model to benchmark.h5\n",
      "327474/327474 [==============================] - 93s - loss: 0.1354 - acc: 0.8128 - val_loss: 0.1418 - val_acc: 0.8031\n",
      "Epoch 18/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1348 - acc: 0.8139Epoch 00017: val_acc did not improve\n",
      "327474/327474 [==============================] - 92s - loss: 0.1348 - acc: 0.8139 - val_loss: 0.1410 - val_acc: 0.8028\n",
      "Epoch 19/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1342 - acc: 0.8150Epoch 00018: val_acc did not improve\n",
      "327474/327474 [==============================] - 96s - loss: 0.1342 - acc: 0.8150 - val_loss: 0.1404 - val_acc: 0.8031\n",
      "Epoch 20/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1336 - acc: 0.8161Epoch 00019: val_acc improved from 0.80312 to 0.80672, saving model to benchmark.h5\n",
      "327474/327474 [==============================] - 94s - loss: 0.1336 - acc: 0.8161 - val_loss: 0.1401 - val_acc: 0.8067\n",
      "Epoch 21/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1331 - acc: 0.8171Epoch 00020: val_acc did not improve\n",
      "327474/327474 [==============================] - 89s - loss: 0.1331 - acc: 0.8170 - val_loss: 0.1395 - val_acc: 0.8054\n",
      "Epoch 22/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1326 - acc: 0.8178Epoch 00021: val_acc improved from 0.80672 to 0.80696, saving model to benchmark.h5\n",
      "327474/327474 [==============================] - 87s - loss: 0.1326 - acc: 0.8178 - val_loss: 0.1392 - val_acc: 0.8070\n",
      "Epoch 23/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1321 - acc: 0.8184Epoch 00022: val_acc improved from 0.80696 to 0.80724, saving model to benchmark.h5\n",
      "327474/327474 [==============================] - 82s - loss: 0.1321 - acc: 0.8184 - val_loss: 0.1387 - val_acc: 0.8072\n",
      "Epoch 24/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1317 - acc: 0.8196Epoch 00023: val_acc did not improve\n",
      "327474/327474 [==============================] - 79s - loss: 0.1317 - acc: 0.8196 - val_loss: 0.1385 - val_acc: 0.8066\n",
      "Epoch 25/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1312 - acc: 0.8204Epoch 00024: val_acc did not improve\n",
      "327474/327474 [==============================] - 78s - loss: 0.1312 - acc: 0.8204 - val_loss: 0.1381 - val_acc: 0.8072\n",
      "Epoch 26/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1308 - acc: 0.8212Epoch 00025: val_acc improved from 0.80724 to 0.80735, saving model to benchmark.h5\n",
      "327474/327474 [==============================] - 80s - loss: 0.1308 - acc: 0.8211 - val_loss: 0.1379 - val_acc: 0.8073\n",
      "Epoch 27/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1304 - acc: 0.8223Epoch 00026: val_acc improved from 0.80735 to 0.80782, saving model to benchmark.h5\n",
      "327474/327474 [==============================] - 76s - loss: 0.1304 - acc: 0.8223 - val_loss: 0.1376 - val_acc: 0.8078\n",
      "Epoch 28/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1300 - acc: 0.8226Epoch 00027: val_acc improved from 0.80782 to 0.80985, saving model to benchmark.h5\n",
      "327474/327474 [==============================] - 78s - loss: 0.1300 - acc: 0.8226 - val_loss: 0.1373 - val_acc: 0.8098\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1296 - acc: 0.8232Epoch 00028: val_acc improved from 0.80985 to 0.81065, saving model to benchmark.h5\n",
      "327474/327474 [==============================] - 75s - loss: 0.1296 - acc: 0.8232 - val_loss: 0.1369 - val_acc: 0.8106\n",
      "Epoch 30/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1292 - acc: 0.8240Epoch 00029: val_acc improved from 0.81065 to 0.81144, saving model to benchmark.h5\n",
      "327474/327474 [==============================] - 74s - loss: 0.1292 - acc: 0.8240 - val_loss: 0.1367 - val_acc: 0.8114\n",
      "Epoch 31/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1289 - acc: 0.8245Epoch 00030: val_acc did not improve\n",
      "327474/327474 [==============================] - 73s - loss: 0.1289 - acc: 0.8245 - val_loss: 0.1367 - val_acc: 0.8097\n",
      "Epoch 32/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.8251Epoch 00031: val_acc improved from 0.81144 to 0.81285, saving model to benchmark.h5\n",
      "327474/327474 [==============================] - 74s - loss: 0.1285 - acc: 0.8251 - val_loss: 0.1367 - val_acc: 0.8128\n",
      "Epoch 33/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1282 - acc: 0.8257Epoch 00032: val_acc improved from 0.81285 to 0.81285, saving model to benchmark.h5\n",
      "327474/327474 [==============================] - 74s - loss: 0.1282 - acc: 0.8256 - val_loss: 0.1361 - val_acc: 0.8128\n",
      "Epoch 34/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1279 - acc: 0.8259Epoch 00033: val_acc did not improve\n",
      "327474/327474 [==============================] - 73s - loss: 0.1279 - acc: 0.8259 - val_loss: 0.1357 - val_acc: 0.8112\n",
      "Epoch 35/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1276 - acc: 0.8265Epoch 00034: val_acc did not improve\n",
      "327474/327474 [==============================] - 74s - loss: 0.1276 - acc: 0.8265 - val_loss: 0.1355 - val_acc: 0.8115\n",
      "Epoch 36/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1273 - acc: 0.8273Epoch 00035: val_acc improved from 0.81285 to 0.81370, saving model to benchmark.h5\n",
      "327474/327474 [==============================] - 73s - loss: 0.1273 - acc: 0.8273 - val_loss: 0.1356 - val_acc: 0.8137\n",
      "Epoch 37/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1270 - acc: 0.8278Epoch 00036: val_acc did not improve\n",
      "327474/327474 [==============================] - 74s - loss: 0.1270 - acc: 0.8279 - val_loss: 0.1352 - val_acc: 0.8119\n",
      "Epoch 38/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1266 - acc: 0.8285Epoch 00037: val_acc improved from 0.81370 to 0.81433, saving model to benchmark.h5\n",
      "327474/327474 [==============================] - 74s - loss: 0.1267 - acc: 0.8284 - val_loss: 0.1353 - val_acc: 0.8143\n",
      "Epoch 39/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1264 - acc: 0.8286Epoch 00038: val_acc did not improve\n",
      "327474/327474 [==============================] - 74s - loss: 0.1264 - acc: 0.8286 - val_loss: 0.1347 - val_acc: 0.8124\n",
      "Epoch 40/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1261 - acc: 0.8291Epoch 00039: val_acc did not improve\n",
      "327474/327474 [==============================] - 72s - loss: 0.1261 - acc: 0.8290 - val_loss: 0.1346 - val_acc: 0.8113\n",
      "Epoch 41/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1258 - acc: 0.8296Epoch 00040: val_acc did not improve\n",
      "327474/327474 [==============================] - 74s - loss: 0.1258 - acc: 0.8297 - val_loss: 0.1344 - val_acc: 0.8114\n",
      "Epoch 42/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1255 - acc: 0.8303Epoch 00041: val_acc did not improve\n",
      "327474/327474 [==============================] - 73s - loss: 0.1256 - acc: 0.8303 - val_loss: 0.1341 - val_acc: 0.8140\n",
      "Epoch 43/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1253 - acc: 0.8306Epoch 00042: val_acc did not improve\n",
      "327474/327474 [==============================] - 74s - loss: 0.1253 - acc: 0.8307 - val_loss: 0.1340 - val_acc: 0.8129\n",
      "Epoch 44/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1250 - acc: 0.8313Epoch 00043: val_acc did not improve\n",
      "327474/327474 [==============================] - 73s - loss: 0.1250 - acc: 0.8313 - val_loss: 0.1338 - val_acc: 0.8143\n",
      "Epoch 45/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1248 - acc: 0.8315Epoch 00044: val_acc improved from 0.81433 to 0.81598, saving model to benchmark.h5\n",
      "327474/327474 [==============================] - 75s - loss: 0.1248 - acc: 0.8315 - val_loss: 0.1339 - val_acc: 0.8160\n",
      "Epoch 46/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1245 - acc: 0.8319Epoch 00045: val_acc did not improve\n",
      "327474/327474 [==============================] - 73s - loss: 0.1245 - acc: 0.8319 - val_loss: 0.1334 - val_acc: 0.8131\n",
      "Epoch 47/100\n",
      "155000/327474 [=============>................] - ETA: 37s - loss: 0.1240 - acc: 0.8329"
     ]
    }
   ],
   "source": [
    "malstm_trained = malstm.fit([x1_train, x2_train], y_train, batch_size=batch_size, \n",
    "                            epochs=n_epoch,verbose=1, validation_split=0.1, \n",
    "                            shuffle=True, callbacks=[checkpoint,estop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on train_set: 0.767615470929\n"
     ]
    }
   ],
   "source": [
    "# we can load our saved weights using load_weights function\n",
    "malstm.load_weights('benchmark.h5')\n",
    "\n",
    "# find the predictions using predict(). This will output only the probability\n",
    "preds  = malstm.predict([x1_train,x2_train])\n",
    "\n",
    "\n",
    "# we need to convert our predicted probabilies to binary classes(0,1), since F1 score doesnot support probabilities\n",
    "predicted_classes = np.zeros((preds.shape[0],1))\n",
    "for i in range(len(preds)):\n",
    "    if preds[i] >= 0.500:\n",
    "        predicted_classes[i]=1\n",
    "        \n",
    "#using sklearn f1_score function to find the score\n",
    "print('F1 Score on train_set: '+ str(f1_score(y_train,predicted_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on test_set: 0.749760700123\n"
     ]
    }
   ],
   "source": [
    "# we can load our saved weights using load_weights function\n",
    "malstm.load_weights('benchmark.h5')\n",
    "\n",
    "# find the predictions using predict(). This will output only the probability\n",
    "preds  = malstm.predict([x1_test,x2_test])\n",
    "\n",
    "# we need to convert our predicted probabilies to binary classes(0,1), since F1 score doesnot support probabilities\n",
    "predicted_classes = np.zeros((preds.shape[0],1))\n",
    "for i in range(len(preds)):\n",
    "    if preds[i] >= 0.500:\n",
    "        predicted_classes[i]=1\n",
    "        \n",
    "#using sklearn f1_score function to find the score\n",
    "print('F1 Score on test_set: '+ str(f1_score(y_test,predicted_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
