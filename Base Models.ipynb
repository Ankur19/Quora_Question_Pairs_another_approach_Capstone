{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "from __future__ import division \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import pickle\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Merge, TimeDistributed, Lambda, Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import backend as K\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.preprocessing import sequence, text\n",
    "\n",
    "from keras.layers import Input, Bidirectional, LSTM, dot, Flatten, Dense, Reshape, add, Dropout, BatchNormalization, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the dataframe\n",
    "df = pd.read_csv('quora_duplicate_questions.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404290, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the data has 404290 rows and 6 columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    255027\n",
       "1    149263\n",
       "Name: is_duplicate, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the positive and negative data\n",
    "df.is_duplicate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "df.drop(['id','qid1','qid2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to clean text\n",
    "\n",
    "def clean_text(text):\n",
    "    ''' Pre process and convert texts to a list of words '''\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"can't\", \"can not\", text)\n",
    "    text = re.sub(r\"n't\", \"not\", text)\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\/\", \" / \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" america \", text)\n",
    "    text = re.sub(r\"\\0s\", \" 0 \", text)\n",
    "    text = re.sub(r\" 9 11 \", \" 911 \", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\" j k \", \" jk \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 404290/404290 [00:48<00:00, 8342.66it/s] \n"
     ]
    }
   ],
   "source": [
    "#Preprocess the data by cleaning the text\n",
    "sents = []\n",
    "for i in tqdm(range(len(df.question1))):\n",
    "    sents.append(clean_text(df.question1[i]))\n",
    "df['question1']  = sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 404290/404290 [01:00<00:00, 6636.49it/s]\n"
     ]
    }
   ],
   "source": [
    "sents = []\n",
    "for i in tqdm(range(len(df.question2))):\n",
    "    sents.append(clean_text(df.question2[i]))\n",
    "df['question2']  = sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is the story of kohinoor koh-i-noor diamond</td>\n",
       "      <td>what would happen if the indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how can i increase the speed of my internet co...</td>\n",
       "      <td>how can internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>why am i mentally very lonely how can i solve it</td>\n",
       "      <td>find the remainder when math 23 ^ 24 / math is...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>which fish would survive in salt water</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0  what is the step by step guide to invest in sh...   \n",
       "1  what is the story of kohinoor koh-i-noor diamond    \n",
       "2  how can i increase the speed of my internet co...   \n",
       "3  why am i mentally very lonely how can i solve it    \n",
       "4  which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  what is the step by step guide to invest in sh...             0  \n",
       "1  what would happen if the indian government sto...             0  \n",
       "2  how can internet speed be increased by hacking...             0  \n",
       "3  find the remainder when math 23 ^ 24 / math is...             0  \n",
       "4            which fish would survive in salt water              0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#saving the cleaned data as a pickle file so that it can be reused\n",
    "import pickle\n",
    "with open ('question_pair.pickle', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('question_pair.pickle', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count the number of words present per question\n",
    "df['q1_count'] = [len(i.split()) for i in df.question1]\n",
    "df['q2_count'] = [len(i.split()) for i in df.question2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmYAAAHjCAYAAACJudN8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYIj/C4jOlCilZiUQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1d787a2650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the number of words present so as to select the max length for the vectors\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.hist(df['q1_count'], bins=60)\n",
    "plt.xlabel('question 1 words count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm0AAAHjCAYAAABxWSiLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2gAAJvD/Azzw8kh1NFKPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1d77e44890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.hist(df['q2_count'], bins=60)\n",
    "plt.xlabel('question 2 words count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.q1_count.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.q2_count.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# considering the visualizations above max_len of 30 should be sufficient for the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tk = text.Tokenizer(num_words=200000)\n",
    "# we use keras Tokenizer to tokenizer the data. \n",
    "# we will only consider top 200000 words that occur in the dataset\n",
    "\n",
    "max_len = 30\n",
    "# the maximum length of each sequence\n",
    "\n",
    "tk.fit_on_texts(list(df.question1.values.astype(str)) + list(df.question2.values.astype(str)))\n",
    "\n",
    "#we now convert the text to numerical data\n",
    "x1 = tk.texts_to_sequences(df.question1.values.astype(str))\n",
    "x2 = tk.texts_to_sequences(df.question2.values.astype(str))\n",
    "\n",
    "# we pad the sequences so that all questions are of the same length(30)\n",
    "x1 = sequence.pad_sequences(x1, maxlen=max_len)\n",
    "x2 = sequence.pad_sequences(x2, maxlen=max_len)\n",
    "\n",
    "\n",
    "word_index = tk.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404290, 30)\n",
      "(404290, 30)\n"
     ]
    }
   ],
   "source": [
    "#x1 and x2 denote the questions that were converted to sequences which can be fed to machine learning algorithms\n",
    "print(x1.shape)\n",
    "print(x2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y is our target variable\n",
    "y = df.is_duplicate.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404290,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we split our data to train and test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x1_train, x1_test, x2_train, x2_test, y_train, y_test = train_test_split(x1,x2,y, test_size = 0.1,random_state=2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [01:59, 18435.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196016 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# we create a dictionary of the words that have pretrained weights\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.840B.300d.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88367/88367 [00:18<00:00, 4658.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# we now create a matrix of weights for the words present n our dataset\n",
    "\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# free some ram\n",
    "#embeddings_index = 0\n",
    "df=0\n",
    "x1=0\n",
    "x2=0\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sav the weight matrix for reuse\n",
    "np.savetxt('embeddings.txt',embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.loadtxt('embeddings.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88368, 300)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model 1\n",
      "Build Model...\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_5 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_6 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 30, 300)       26510400    input_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)          (None, 30, 300)       26510400    input_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistribu (None, 30, 300)       90300       embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistribu (None, 30, 300)       90300       embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (None, 300)           0           time_distributed_1[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)                (None, 300)           0           time_distributed_2[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "add_1 (Add)                      (None, 300)           0           lambda_1[0][0]                   \n",
      "                                                                   lambda_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 300)           90300       add_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 300)           0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 300)           1200        dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 300)           90300       batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 300)           0           dense_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 300)           1200        dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 1)             301         batch_normalization_2[0][0]      \n",
      "====================================================================================================\n",
      "Total params: 53,384,701\n",
      "Trainable params: 362,701\n",
      "Non-trainable params: 53,022,000\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Base Model 1')\n",
    "print('Build Model...')\n",
    "\n",
    "ques1 = Input(shape=(30,))\n",
    "ques2 = Input(shape=(30,))\n",
    "q1 = Embedding(len(word_index) + 1, \n",
    "                 300, \n",
    "                 weights=[embedding_matrix], \n",
    "                 input_length=30, \n",
    "                 trainable=False)(ques1)\n",
    "\n",
    "q1 = TimeDistributed(Dense(300, activation='relu'))(q1)\n",
    "q1 = Lambda(lambda x: K.sum(x, axis=1), output_shape=(300,))(q1)\n",
    "\n",
    "\n",
    "q2 = Embedding(len(word_index) + 1, \n",
    "                 300, \n",
    "                 weights=[embedding_matrix], \n",
    "                 input_length=30, \n",
    "                 trainable=False)(ques2)\n",
    "\n",
    "q2 = TimeDistributed(Dense(300, activation='relu'))(q2)\n",
    "q2 = Lambda(lambda x: K.sum(x, axis=1), output_shape=(300,))(q2)\n",
    "\n",
    "\n",
    "merged = add([q1,q2])\n",
    "merged = Dense(300, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(300, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model(inputs=[ques1,ques2], outputs=merged)\n",
    "\n",
    "checkpoint = ModelCheckpoint('base 1.h5', monitor='val_acc', save_best_only=True, verbose=2)\n",
    "\n",
    "estop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=20, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 327474 samples, validate on 36387 samples\n",
      "Epoch 1/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.5494 - acc: 0.7167Epoch 00000: val_acc improved from -inf to 0.74491, saving model to base 1.h5\n",
      "327474/327474 [==============================] - 13s - loss: 0.5491 - acc: 0.7168 - val_loss: 0.5052 - val_acc: 0.7449\n",
      "Epoch 2/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.4782 - acc: 0.7614Epoch 00001: val_acc improved from 0.74491 to 0.76959, saving model to base 1.h5\n",
      "327474/327474 [==============================] - 12s - loss: 0.4781 - acc: 0.7614 - val_loss: 0.4644 - val_acc: 0.7696\n",
      "Epoch 3/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.4484 - acc: 0.7802Epoch 00002: val_acc improved from 0.76959 to 0.77731, saving model to base 1.h5\n",
      "327474/327474 [==============================] - 12s - loss: 0.4485 - acc: 0.7802 - val_loss: 0.4560 - val_acc: 0.7773\n",
      "Epoch 4/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.4286 - acc: 0.7935Epoch 00003: val_acc improved from 0.77731 to 0.78718, saving model to base 1.h5\n",
      "327474/327474 [==============================] - 12s - loss: 0.4285 - acc: 0.7936 - val_loss: 0.4363 - val_acc: 0.7872\n",
      "Epoch 5/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.4125 - acc: 0.8021Epoch 00004: val_acc improved from 0.78718 to 0.79091, saving model to base 1.h5\n",
      "327474/327474 [==============================] - 13s - loss: 0.4125 - acc: 0.8021 - val_loss: 0.4316 - val_acc: 0.7909\n",
      "Epoch 6/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.3981 - acc: 0.8123Epoch 00005: val_acc improved from 0.79091 to 0.79603, saving model to base 1.h5\n",
      "327474/327474 [==============================] - 13s - loss: 0.3981 - acc: 0.8123 - val_loss: 0.4229 - val_acc: 0.7960\n",
      "Epoch 7/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.3859 - acc: 0.8190Epoch 00006: val_acc did not improve\n",
      "327474/327474 [==============================] - 13s - loss: 0.3859 - acc: 0.8190 - val_loss: 0.4265 - val_acc: 0.7942\n",
      "Epoch 8/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.3741 - acc: 0.8251- ETA: 0s - loss: 0.3737 Epoch 00007: val_acc improved from 0.79603 to 0.79924, saving model to base 1.h5\n",
      "327474/327474 [==============================] - 14s - loss: 0.3741 - acc: 0.8251 - val_loss: 0.4218 - val_acc: 0.7992\n",
      "Epoch 9/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.3637 - acc: 0.8316Epoch 00008: val_acc improved from 0.79924 to 0.80372, saving model to base 1.h5\n",
      "327474/327474 [==============================] - 14s - loss: 0.3637 - acc: 0.8316 - val_loss: 0.4145 - val_acc: 0.8037\n",
      "Epoch 10/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.3533 - acc: 0.8381Epoch 00009: val_acc did not improve\n",
      "327474/327474 [==============================] - 13s - loss: 0.3534 - acc: 0.8381 - val_loss: 0.4199 - val_acc: 0.8024\n",
      "Epoch 11/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.3445 - acc: 0.8435Epoch 00010: val_acc did not improve\n",
      "327474/327474 [==============================] - 13s - loss: 0.3445 - acc: 0.8435 - val_loss: 0.4260 - val_acc: 0.8020\n",
      "Epoch 12/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.3378 - acc: 0.8468Epoch 00011: val_acc improved from 0.80372 to 0.80614, saving model to base 1.h5\n",
      "327474/327474 [==============================] - 14s - loss: 0.3378 - acc: 0.8468 - val_loss: 0.4122 - val_acc: 0.8061\n",
      "Epoch 13/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.3292 - acc: 0.8515Epoch 00012: val_acc improved from 0.80614 to 0.80817, saving model to base 1.h5\n",
      "327474/327474 [==============================] - 17s - loss: 0.3291 - acc: 0.8516 - val_loss: 0.4119 - val_acc: 0.8082\n",
      "Epoch 14/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.3218 - acc: 0.8550Epoch 00013: val_acc improved from 0.80817 to 0.80963, saving model to base 1.h5\n",
      "327474/327474 [==============================] - 14s - loss: 0.3218 - acc: 0.8550 - val_loss: 0.4105 - val_acc: 0.8096\n",
      "Epoch 15/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.3145 - acc: 0.8597Epoch 00014: val_acc improved from 0.80963 to 0.81015, saving model to base 1.h5\n",
      "327474/327474 [==============================] - 16s - loss: 0.3145 - acc: 0.8597 - val_loss: 0.4121 - val_acc: 0.8102\n",
      "Epoch 16/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.3074 - acc: 0.8637Epoch 00015: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.3075 - acc: 0.8637 - val_loss: 0.4219 - val_acc: 0.8100\n",
      "Epoch 17/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.3010 - acc: 0.8662Epoch 00016: val_acc did not improve\n",
      "327474/327474 [==============================] - 13s - loss: 0.3010 - acc: 0.8662 - val_loss: 0.4236 - val_acc: 0.8070\n",
      "Epoch 18/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2958 - acc: 0.8695- ETEpoch 00017: val_acc did not improve\n",
      "327474/327474 [==============================] - 16s - loss: 0.2958 - acc: 0.8695 - val_loss: 0.4126 - val_acc: 0.8098\n",
      "Epoch 19/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.2899 - acc: 0.8728Epoch 00018: val_acc improved from 0.81015 to 0.81098, saving model to base 1.h5\n",
      "327474/327474 [==============================] - 18s - loss: 0.2899 - acc: 0.8728 - val_loss: 0.4133 - val_acc: 0.8110\n",
      "Epoch 20/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2845 - acc: 0.8753Epoch 00019: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.2845 - acc: 0.8753 - val_loss: 0.4231 - val_acc: 0.8108\n",
      "Epoch 21/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.2797 - acc: 0.8775Epoch 00020: val_acc did not improve\n",
      "327474/327474 [==============================] - 16s - loss: 0.2798 - acc: 0.8775 - val_loss: 0.4248 - val_acc: 0.8098\n",
      "Epoch 22/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2751 - acc: 0.8795Epoch 00021: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.2751 - acc: 0.8796 - val_loss: 0.4235 - val_acc: 0.8094\n",
      "Epoch 23/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.2708 - acc: 0.8814Epoch 00022: val_acc improved from 0.81098 to 0.81177, saving model to base 1.h5\n",
      "327474/327474 [==============================] - 16s - loss: 0.2709 - acc: 0.8813 - val_loss: 0.4264 - val_acc: 0.8118\n",
      "Epoch 24/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.2657 - acc: 0.8849Epoch 00023: val_acc did not improve\n",
      "327474/327474 [==============================] - 16s - loss: 0.2657 - acc: 0.8849 - val_loss: 0.4797 - val_acc: 0.8025\n",
      "Epoch 25/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2611 - acc: 0.8875Epoch 00024: val_acc improved from 0.81177 to 0.81587, saving model to base 1.h5\n",
      "327474/327474 [==============================] - 18s - loss: 0.2611 - acc: 0.8875 - val_loss: 0.4290 - val_acc: 0.8159\n",
      "Epoch 26/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.2578 - acc: 0.8888Epoch 00025: val_acc did not improve\n",
      "327474/327474 [==============================] - 16s - loss: 0.2579 - acc: 0.8887 - val_loss: 0.4296 - val_acc: 0.8102\n",
      "Epoch 27/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2540 - acc: 0.8909Epoch 00026: val_acc did not improve\n",
      "327474/327474 [==============================] - 15s - loss: 0.2540 - acc: 0.8909 - val_loss: 0.4408 - val_acc: 0.8111\n",
      "Epoch 28/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2505 - acc: 0.8923- ETA: 1s - lossEpoch 00027: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.2505 - acc: 0.8923 - val_loss: 0.4567 - val_acc: 0.8118\n",
      "Epoch 29/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.2465 - acc: 0.8941Epoch 00028: val_acc did not improve\n",
      "327474/327474 [==============================] - 17s - loss: 0.2466 - acc: 0.8941 - val_loss: 0.4462 - val_acc: 0.8117\n",
      "Epoch 30/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.2438 - acc: 0.8955Epoch 00029: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.2438 - acc: 0.8954 - val_loss: 0.4401 - val_acc: 0.8132\n",
      "Epoch 31/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2405 - acc: 0.8967Epoch 00030: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.2405 - acc: 0.8967 - val_loss: 0.4515 - val_acc: 0.8141\n",
      "Epoch 32/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2375 - acc: 0.8983Epoch 00031: val_acc did not improve\n",
      "327474/327474 [==============================] - 20s - loss: 0.2375 - acc: 0.8983 - val_loss: 0.4433 - val_acc: 0.8129\n",
      "Epoch 33/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2346 - acc: 0.8997Epoch 00032: val_acc did not improve\n",
      "327474/327474 [==============================] - 21s - loss: 0.2347 - acc: 0.8997 - val_loss: 0.4608 - val_acc: 0.8076\n",
      "Epoch 34/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.2310 - acc: 0.9019Epoch 00033: val_acc did not improve\n",
      "327474/327474 [==============================] - 18s - loss: 0.2312 - acc: 0.9018 - val_loss: 0.4612 - val_acc: 0.8119\n",
      "Epoch 35/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.2290 - acc: 0.9024Epoch 00034: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.2290 - acc: 0.9024 - val_loss: 0.4543 - val_acc: 0.8148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2c712e8290>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x1_train, x2_train], y=y_train, batch_size=500, epochs=100,\n",
    "verbose=1, validation_split=0.1, shuffle=True, callbacks=[checkpoint,estop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18048"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 81.5% validation accuracy\n",
    "del q1\n",
    "del q2\n",
    "del merged\n",
    "del model\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model 2\n",
      "Build Model...\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_4 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 30, 300)       26510400    input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)          (None, 30, 300)       26510400    input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistribu (None, 30, 300)       90300       embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistribu (None, 30, 300)       90300       embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (None, 300)           0           time_distributed_1[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)                (None, 300)           0           time_distributed_2[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "add_1 (Add)                      (None, 300)           0           lambda_1[0][0]                   \n",
      "                                                                   lambda_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 300)           90300       add_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 300)           0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 300)           1200        dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 300)           90300       batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 300)           0           dense_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 300)           1200        dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 1)             301         batch_normalization_2[0][0]      \n",
      "====================================================================================================\n",
      "Total params: 53,384,701\n",
      "Trainable params: 362,701\n",
      "Non-trainable params: 53,022,000\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Base Model 2')\n",
    "print('Build Model...')\n",
    "\n",
    "ques1 = Input(shape=(30,))\n",
    "ques2 = Input(shape=(30,))\n",
    "q1 = Embedding(len(word_index) + 1, \n",
    "                 300, \n",
    "                 weights=[embedding_matrix], \n",
    "                 input_length=30, \n",
    "                 trainable=False)(ques1)\n",
    "\n",
    "q1 = TimeDistributed(Dense(300, activation='relu'))(q1)\n",
    "q1 = Lambda(lambda x: K.max(x, axis=1), output_shape=(300,))(q1)\n",
    "\n",
    "\n",
    "q2 = Embedding(len(word_index) + 1, \n",
    "                 300, \n",
    "                 weights=[embedding_matrix], \n",
    "                 input_length=30, \n",
    "                 trainable=False)(ques2)\n",
    "\n",
    "q2 = TimeDistributed(Dense(300, activation='relu'))(q2)\n",
    "q2 = Lambda(lambda x: K.max(x, axis=1), output_shape=(300,))(q2)\n",
    "\n",
    "\n",
    "merged = add([q1,q2])\n",
    "merged = Dense(300, activation='relu')(merged)\n",
    "merged = Dropout(0.3)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(300, activation='relu')(merged)\n",
    "merged = Dropout(0.3)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model(inputs=[ques1,ques2], outputs=merged)\n",
    "\n",
    "checkpoint = ModelCheckpoint('base 2.h5', monitor='val_acc', save_best_only=True, verbose=2)\n",
    "\n",
    "estop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=20, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 327474 samples, validate on 36387 samples\n",
      "Epoch 1/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.5608 - acc: 0.7146Epoch 00000: val_acc improved from -inf to 0.74889, saving model to base 2.h5\n",
      "327474/327474 [==============================] - 16s - loss: 0.5607 - acc: 0.7146 - val_loss: 0.5116 - val_acc: 0.7489\n",
      "Epoch 2/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.4806 - acc: 0.7621Epoch 00001: val_acc improved from 0.74889 to 0.76242, saving model to base 2.h5\n",
      "327474/327474 [==============================] - 15s - loss: 0.4806 - acc: 0.7622 - val_loss: 0.4721 - val_acc: 0.7624\n",
      "Epoch 3/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.4452 - acc: 0.7825Epoch 00002: val_acc improved from 0.76242 to 0.77640, saving model to base 2.h5\n",
      "327474/327474 [==============================] - 15s - loss: 0.4452 - acc: 0.7826 - val_loss: 0.4505 - val_acc: 0.7764\n",
      "Epoch 4/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.4181 - acc: 0.7982Epoch 00003: val_acc improved from 0.77640 to 0.78550, saving model to base 2.h5\n",
      "327474/327474 [==============================] - 15s - loss: 0.4181 - acc: 0.7981 - val_loss: 0.4392 - val_acc: 0.7855\n",
      "Epoch 5/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.3942 - acc: 0.8121Epoch 00004: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.3942 - acc: 0.8122 - val_loss: 0.5449 - val_acc: 0.7541\n",
      "Epoch 6/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.3735 - acc: 0.8243Epoch 00005: val_acc improved from 0.78550 to 0.78643, saving model to base 2.h5\n",
      "327474/327474 [==============================] - 15s - loss: 0.3735 - acc: 0.8243 - val_loss: 0.4344 - val_acc: 0.7864\n",
      "Epoch 7/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.3560 - acc: 0.8339Epoch 00006: val_acc improved from 0.78643 to 0.78709, saving model to base 2.h5\n",
      "327474/327474 [==============================] - 15s - loss: 0.3560 - acc: 0.8339 - val_loss: 0.4349 - val_acc: 0.7871\n",
      "Epoch 8/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.3408 - acc: 0.8428Epoch 00007: val_acc improved from 0.78709 to 0.79831, saving model to base 2.h5\n",
      "327474/327474 [==============================] - 15s - loss: 0.3408 - acc: 0.8428 - val_loss: 0.4196 - val_acc: 0.7983\n",
      "Epoch 9/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.3280 - acc: 0.8498Epoch 00008: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.3280 - acc: 0.8498 - val_loss: 0.4477 - val_acc: 0.7719\n",
      "Epoch 10/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.3154 - acc: 0.8562Epoch 00009: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.3155 - acc: 0.8562 - val_loss: 0.4201 - val_acc: 0.7939\n",
      "Epoch 11/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.3046 - acc: 0.8626Epoch 00010: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.3046 - acc: 0.8626 - val_loss: 0.4312 - val_acc: 0.7982\n",
      "Epoch 12/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.2941 - acc: 0.8673Epoch 00011: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.2942 - acc: 0.8673 - val_loss: 0.4548 - val_acc: 0.7671\n",
      "Epoch 13/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2845 - acc: 0.8727Epoch 00012: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.2845 - acc: 0.8727 - val_loss: 0.4754 - val_acc: 0.7532\n",
      "Epoch 14/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.2754 - acc: 0.8772Epoch 00013: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.2754 - acc: 0.8772 - val_loss: 0.4619 - val_acc: 0.7656\n",
      "Epoch 15/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2668 - acc: 0.8822Epoch 00014: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.2669 - acc: 0.8822 - val_loss: 0.4338 - val_acc: 0.7863\n",
      "Epoch 16/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.2601 - acc: 0.8854Epoch 00015: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.2601 - acc: 0.8854 - val_loss: 0.4237 - val_acc: 0.7917\n",
      "Epoch 17/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2531 - acc: 0.8883Epoch 00016: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.2531 - acc: 0.8883 - val_loss: 0.4609 - val_acc: 0.7658\n",
      "Epoch 18/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2456 - acc: 0.8922Epoch 00017: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.2456 - acc: 0.8921 - val_loss: 0.4642 - val_acc: 0.7654\n",
      "Epoch 19/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2396 - acc: 0.8955Epoch 00018: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.2397 - acc: 0.8954 - val_loss: 0.4886 - val_acc: 0.7502\n",
      "Epoch 20/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.2347 - acc: 0.8974Epoch 00019: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.2347 - acc: 0.8974 - val_loss: 0.4431 - val_acc: 0.7815\n",
      "Epoch 21/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.2286 - acc: 0.9008Epoch 00020: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.2286 - acc: 0.9008 - val_loss: 0.4511 - val_acc: 0.7844\n",
      "Epoch 22/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9027Epoch 00021: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.2244 - acc: 0.9027 - val_loss: 0.4418 - val_acc: 0.7897\n",
      "Epoch 23/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.2185 - acc: 0.9056Epoch 00022: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.2186 - acc: 0.9056 - val_loss: 0.4623 - val_acc: 0.7781\n",
      "Epoch 24/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.2132 - acc: 0.9084Epoch 00023: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.2132 - acc: 0.9084 - val_loss: 0.5051 - val_acc: 0.7565\n",
      "Epoch 25/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.2101 - acc: 0.9101Epoch 00024: val_acc did not improve\n",
      "327474/327474 [==============================] - 15s - loss: 0.2101 - acc: 0.9101 - val_loss: 0.4540 - val_acc: 0.7808\n",
      "Epoch 26/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.2054 - acc: 0.9124Epoch 00025: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.2054 - acc: 0.9124 - val_loss: 0.4551 - val_acc: 0.7823\n",
      "Epoch 27/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.2010 - acc: 0.9145Epoch 00026: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.2010 - acc: 0.9145 - val_loss: 0.4699 - val_acc: 0.7809\n",
      "Epoch 28/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1982 - acc: 0.9159Epoch 00027: val_acc did not improve\n",
      "327474/327474 [==============================] - 15s - loss: 0.1982 - acc: 0.9158 - val_loss: 0.4811 - val_acc: 0.7929\n",
      "Epoch 29/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.1935 - acc: 0.9180Epoch 00028: val_acc improved from 0.79831 to 0.80232, saving model to base 2.h5\n",
      "327474/327474 [==============================] - 15s - loss: 0.1934 - acc: 0.9180 - val_loss: 0.4602 - val_acc: 0.8023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1d75616b50>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x1_train, x2_train], y=y_train, batch_size=500, epochs=100,\n",
    "verbose=1, validation_split=0.1, shuffle=True, callbacks=[checkpoint,estop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 80.2% accuracy\n",
    "#Taking max in the lambda layer instead of sum and increasing the dropout seems to have made a bad efect.\n",
    "\n",
    "del q1\n",
    "del q2\n",
    "del merged\n",
    "del model\n",
    "K.clear_session()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model 3\n",
      "Build Model...\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_4 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 30, 300)       26510400    input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)          (None, 30, 300)       26510400    input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistribu (None, 30, 300)       90300       embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistribu (None, 30, 300)       90300       embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (None, 300)           0           time_distributed_1[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)                (None, 300)           0           time_distributed_2[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "add_1 (Add)                      (None, 300)           0           lambda_1[0][0]                   \n",
      "                                                                   lambda_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 300)           1200        add_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 300)           90300       batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 300)           0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 300)           1200        dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 300)           90300       batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 300)           0           dense_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 300)           1200        dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 1)             301         batch_normalization_3[0][0]      \n",
      "====================================================================================================\n",
      "Total params: 53,385,901\n",
      "Trainable params: 363,301\n",
      "Non-trainable params: 53,022,600\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Base Model 3')\n",
    "print('Build Model...')\n",
    "\n",
    "ques1 = Input(shape=(30,))\n",
    "ques2 = Input(shape=(30,))\n",
    "q1 = Embedding(len(word_index) + 1, \n",
    "                 300, \n",
    "                 weights=[embedding_matrix], \n",
    "                 input_length=30, \n",
    "                 trainable=False)(ques1)\n",
    "\n",
    "q1 = TimeDistributed(Dense(300, activation='relu'))(q1)\n",
    "q1 = Lambda(lambda x: K.max(x, axis=1), output_shape=(300,))(q1)\n",
    "\n",
    "\n",
    "q2 = Embedding(len(word_index) + 1, \n",
    "                 300, \n",
    "                 weights=[embedding_matrix], \n",
    "                 input_length=30, \n",
    "                 trainable=False)(ques2)\n",
    "\n",
    "q2 = TimeDistributed(Dense(300, activation='relu'))(q2)\n",
    "q2 = Lambda(lambda x: K.max(x, axis=1), output_shape=(300,))(q2)\n",
    "\n",
    "\n",
    "merged = add([q1,q2])\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(300, activation='relu')(merged)\n",
    "merged = Dropout(0.1)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(300, activation='relu')(merged)\n",
    "merged = Dropout(0.1)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model(inputs=[ques1,ques2], outputs=merged)\n",
    "\n",
    "checkpoint = ModelCheckpoint('base 3.h5', monitor='val_acc', save_best_only=True, verbose=2)\n",
    "\n",
    "estop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=20, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 327474 samples, validate on 36387 samples\n",
      "Epoch 1/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.5032 - acc: 0.7474Epoch 00000: val_acc improved from -inf to 0.78621, saving model to base 3.h5\n",
      "327474/327474 [==============================] - 16s - loss: 0.5029 - acc: 0.7476 - val_loss: 0.4351 - val_acc: 0.7862\n",
      "Epoch 2/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.4159 - acc: 0.7992- ETA: 5s - loss: 0. - ETA: 1sEpoch 00001: val_acc improved from 0.78621 to 0.80078, saving model to base 3.h5\n",
      "327474/327474 [==============================] - 15s - loss: 0.4159 - acc: 0.7992 - val_loss: 0.4110 - val_acc: 0.8008\n",
      "Epoch 3/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.3754 - acc: 0.8233Epoch 00002: val_acc improved from 0.80078 to 0.80765, saving model to base 3.h5\n",
      "327474/327474 [==============================] - 15s - loss: 0.3754 - acc: 0.8233 - val_loss: 0.4031 - val_acc: 0.8077\n",
      "Epoch 4/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.3432 - acc: 0.8423Epoch 00003: val_acc improved from 0.80765 to 0.81394, saving model to base 3.h5\n",
      "327474/327474 [==============================] - 15s - loss: 0.3432 - acc: 0.8423 - val_loss: 0.3949 - val_acc: 0.8139\n",
      "Epoch 5/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.3159 - acc: 0.8571Epoch 00004: val_acc improved from 0.81394 to 0.81809, saving model to base 3.h5\n",
      "327474/327474 [==============================] - 15s - loss: 0.3160 - acc: 0.8570 - val_loss: 0.3981 - val_acc: 0.8181\n",
      "Epoch 6/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.2916 - acc: 0.8693Epoch 00005: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.2918 - acc: 0.8692 - val_loss: 0.4020 - val_acc: 0.8178\n",
      "Epoch 7/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.2713 - acc: 0.8797Epoch 00006: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.2713 - acc: 0.8797 - val_loss: 0.4063 - val_acc: 0.8178\n",
      "Epoch 8/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2510 - acc: 0.8901Epoch 00007: val_acc improved from 0.81809 to 0.82035, saving model to base 3.h5\n",
      "327474/327474 [==============================] - 15s - loss: 0.2511 - acc: 0.8900 - val_loss: 0.4238 - val_acc: 0.8203\n",
      "Epoch 9/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.2340 - acc: 0.8984- ETA: 1s - loss: 0.233Epoch 00008: val_acc did not improve\n",
      "327474/327474 [==============================] - 15s - loss: 0.2342 - acc: 0.8983 - val_loss: 0.4362 - val_acc: 0.8173\n",
      "Epoch 10/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2188 - acc: 0.9059Epoch 00009: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.2188 - acc: 0.9059 - val_loss: 0.4486 - val_acc: 0.8200\n",
      "Epoch 11/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2050 - acc: 0.9119Epoch 00010: val_acc improved from 0.82035 to 0.82062, saving model to base 3.h5\n",
      "327474/327474 [==============================] - 15s - loss: 0.2051 - acc: 0.9119 - val_loss: 0.4652 - val_acc: 0.8206\n",
      "Epoch 12/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.1931 - acc: 0.9177Epoch 00011: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.1932 - acc: 0.9177 - val_loss: 0.4761 - val_acc: 0.8203\n",
      "Epoch 13/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.1828 - acc: 0.9230Epoch 00012: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.1829 - acc: 0.9230 - val_loss: 0.4979 - val_acc: 0.8192\n",
      "Epoch 14/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1734 - acc: 0.9271Epoch 00013: val_acc improved from 0.82062 to 0.82365, saving model to base 3.h5\n",
      "327474/327474 [==============================] - 15s - loss: 0.1733 - acc: 0.9272 - val_loss: 0.5170 - val_acc: 0.8236\n",
      "Epoch 15/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.1635 - acc: 0.9317Epoch 00014: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.1637 - acc: 0.9315 - val_loss: 0.5228 - val_acc: 0.8171\n",
      "Epoch 16/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1560 - acc: 0.9349Epoch 00015: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.1561 - acc: 0.9349 - val_loss: 0.5151 - val_acc: 0.8200\n",
      "Epoch 17/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.1497 - acc: 0.9383- ETA: 3s - loss: 0 - ETA: 2s  - ETA: 0s - loss: 0.1489 -Epoch 00016: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.1497 - acc: 0.9383 - val_loss: 0.5441 - val_acc: 0.8211\n",
      "Epoch 18/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.1437 - acc: 0.9409Epoch 00017: val_acc improved from 0.82365 to 0.82411, saving model to base 3.h5\n",
      "327474/327474 [==============================] - 15s - loss: 0.1437 - acc: 0.9409 - val_loss: 0.5500 - val_acc: 0.8241\n",
      "Epoch 19/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1377 - acc: 0.9434Epoch 00018: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.1377 - acc: 0.9434 - val_loss: 0.5592 - val_acc: 0.8239\n",
      "Epoch 20/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.1321 - acc: 0.9460Epoch 00019: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.1321 - acc: 0.9460 - val_loss: 0.5650 - val_acc: 0.8238\n",
      "Epoch 21/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.1282 - acc: 0.9478- ETA: Epoch 00020: val_acc improved from 0.82411 to 0.82469, saving model to base 3.h5\n",
      "327474/327474 [==============================] - 15s - loss: 0.1283 - acc: 0.9478 - val_loss: 0.5876 - val_acc: 0.8247\n",
      "Epoch 22/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.1247 - acc: 0.9487Epoch 00021: val_acc improved from 0.82469 to 0.82763, saving model to base 3.h5\n",
      "327474/327474 [==============================] - 15s - loss: 0.1249 - acc: 0.9486 - val_loss: 0.5889 - val_acc: 0.8276\n",
      "Epoch 23/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.1194 - acc: 0.9517Epoch 00022: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.1195 - acc: 0.9516 - val_loss: 0.5909 - val_acc: 0.8252\n",
      "Epoch 24/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9537Epoch 00023: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.1152 - acc: 0.9536 - val_loss: 0.6145 - val_acc: 0.8240\n",
      "Epoch 25/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.1126 - acc: 0.9546Epoch 00024: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.1127 - acc: 0.9546 - val_loss: 0.6077 - val_acc: 0.8226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efe23e35f90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x1_train, x2_train], y=y_train, batch_size=500, epochs=100,\n",
    "verbose=1, validation_split=0.1, shuffle=True, callbacks=[checkpoint,estop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 82.7% accuracy\n",
    "#Taking max in the lambda layer instead of sum and decreasing the dropout works like a charm.\n",
    "\n",
    "del q1\n",
    "del q2\n",
    "del merged\n",
    "del model\n",
    "K.clear_session()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base 4\n",
      "Build model . . \n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 30, 300)       26510400    input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 30, 300)       26510400    input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)                (None, 30, 64)        38464       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)                (None, 30, 64)        38464       embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 30, 64)        0           conv1d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 30, 64)        0           conv1d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glob (None, 64)            0           dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glob (None, 64)            0           dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 128)           0           global_average_pooling1d_1[0][0] \n",
      "                                                                   global_average_pooling1d_2[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 128)           512         concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 128)           16512       batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 128)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 128)           512         dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 64)            8256        batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 64)            0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 64)            256         dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1)             65          batch_normalization_3[0][0]      \n",
      "====================================================================================================\n",
      "Total params: 53,123,841\n",
      "Trainable params: 102,401\n",
      "Non-trainable params: 53,021,440\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Base 4')\n",
    "\n",
    "print(\"Build model . . \")\n",
    "\n",
    "ques1 = Input(shape=(30,))\n",
    "ques2 = Input(shape=(30,))\n",
    "q1 = Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], input_length=30, trainable=False)(ques1)\n",
    "q1 = Conv1D(filters = 64,kernel_size=2,strides = 1,padding='same', activation='relu')(q1)\n",
    "q1 = Dropout(0.2)(q1)\n",
    "q1 = GlobalAveragePooling1D()(q1)\n",
    "\n",
    "q2 = Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], input_length=30, trainable=False)(ques2)\n",
    "q2 = Conv1D(filters = 64,kernel_size=2,strides = 1,padding='same', activation='relu')(q2)\n",
    "q2 = Dropout(0.2)(q2)\n",
    "q2 = GlobalAveragePooling1D()(q2)\n",
    "\n",
    "\n",
    "#merge1 = add([q1,q2])\n",
    "#merge2 = subtract([q1,q2])\n",
    "merged = concatenate([q1,q2])\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(128, activation='relu')(merged)\n",
    "merged = Dropout(0.1)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(64, activation='relu')(merged)\n",
    "merged = Dropout(0.1)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model(inputs=[ques1,ques2], outputs=merged)\n",
    "\n",
    "checkpoint = ModelCheckpoint('base 4.h5', monitor='val_acc', save_best_only=True, verbose=2)\n",
    "\n",
    "estop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=20, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 327474 samples, validate on 36387 samples\n",
      "Epoch 1/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.5407 - acc: 0.7205Epoch 00000: val_acc improved from -inf to 0.75763, saving model to base 4.h5\n",
      "327474/327474 [==============================] - 14s - loss: 0.5407 - acc: 0.7205 - val_loss: 0.4892 - val_acc: 0.7576\n",
      "Epoch 2/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.4775 - acc: 0.7620Epoch 00001: val_acc improved from 0.75763 to 0.77506, saving model to base 4.h5\n",
      "327474/327474 [==============================] - 14s - loss: 0.4775 - acc: 0.7620 - val_loss: 0.4523 - val_acc: 0.7751\n",
      "Epoch 3/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.4523 - acc: 0.7767Epoch 00002: val_acc improved from 0.77506 to 0.78171, saving model to base 4.h5\n",
      "327474/327474 [==============================] - 14s - loss: 0.4523 - acc: 0.7767 - val_loss: 0.4416 - val_acc: 0.7817\n",
      "Epoch 4/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.4357 - acc: 0.7871Epoch 00003: val_acc improved from 0.78171 to 0.79031, saving model to base 4.h5\n",
      "327474/327474 [==============================] - 14s - loss: 0.4356 - acc: 0.7871 - val_loss: 0.4277 - val_acc: 0.7903\n",
      "Epoch 5/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.4214 - acc: 0.7955Epoch 00004: val_acc improved from 0.79031 to 0.79163, saving model to base 4.h5\n",
      "327474/327474 [==============================] - 14s - loss: 0.4214 - acc: 0.7956 - val_loss: 0.4246 - val_acc: 0.7916\n",
      "Epoch 6/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.4129 - acc: 0.8009Epoch 00005: val_acc improved from 0.79163 to 0.79831, saving model to base 4.h5\n",
      "327474/327474 [==============================] - 14s - loss: 0.4129 - acc: 0.8008 - val_loss: 0.4150 - val_acc: 0.7983\n",
      "Epoch 7/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.4031 - acc: 0.8066Epoch 00006: val_acc improved from 0.79831 to 0.80012, saving model to base 4.h5\n",
      "327474/327474 [==============================] - 15s - loss: 0.4031 - acc: 0.8066 - val_loss: 0.4106 - val_acc: 0.8001\n",
      "Epoch 8/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.3967 - acc: 0.8107- ETA: 1s - loss:Epoch 00007: val_acc improved from 0.80012 to 0.80152, saving model to base 4.h5\n",
      "327474/327474 [==============================] - 15s - loss: 0.3967 - acc: 0.8106 - val_loss: 0.4079 - val_acc: 0.8015\n",
      "Epoch 9/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.3902 - acc: 0.8149Epoch 00008: val_acc did not improve\n",
      "327474/327474 [==============================] - 14s - loss: 0.3902 - acc: 0.8148 - val_loss: 0.4101 - val_acc: 0.7994\n",
      "Epoch 10/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.3842 - acc: 0.8182Epoch 00009: val_acc improved from 0.80152 to 0.80521, saving model to base 4.h5\n",
      "327474/327474 [==============================] - 16s - loss: 0.3842 - acc: 0.8182 - val_loss: 0.4019 - val_acc: 0.8052\n",
      "Epoch 11/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.3791 - acc: 0.8210Epoch 00010: val_acc improved from 0.80521 to 0.80727, saving model to base 4.h5\n",
      "327474/327474 [==============================] - 18s - loss: 0.3791 - acc: 0.8210 - val_loss: 0.4010 - val_acc: 0.8073\n",
      "Epoch 12/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.3750 - acc: 0.8240Epoch 00011: val_acc improved from 0.80727 to 0.80859, saving model to base 4.h5\n",
      "327474/327474 [==============================] - 15s - loss: 0.3750 - acc: 0.8240 - val_loss: 0.3985 - val_acc: 0.8086\n",
      "Epoch 13/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.3715 - acc: 0.8251Epoch 00012: val_acc did not improve\n",
      "327474/327474 [==============================] - 18s - loss: 0.3716 - acc: 0.8251 - val_loss: 0.4118 - val_acc: 0.7998\n",
      "Epoch 14/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.3678 - acc: 0.8278Epoch 00013: val_acc did not improve\n",
      "327474/327474 [==============================] - 17s - loss: 0.3678 - acc: 0.8278 - val_loss: 0.4033 - val_acc: 0.8041\n",
      "Epoch 15/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.3636 - acc: 0.8305Epoch 00014: val_acc did not improve\n",
      "327474/327474 [==============================] - 16s - loss: 0.3636 - acc: 0.8305 - val_loss: 0.4013 - val_acc: 0.8048\n",
      "Epoch 16/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.3604 - acc: 0.8319Epoch 00015: val_acc did not improve\n",
      "327474/327474 [==============================] - 17s - loss: 0.3603 - acc: 0.8319 - val_loss: 0.3998 - val_acc: 0.8067\n",
      "Epoch 17/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.3580 - acc: 0.8324Epoch 00016: val_acc improved from 0.80859 to 0.81007, saving model to base 4.h5\n",
      "327474/327474 [==============================] - 18s - loss: 0.3580 - acc: 0.8324 - val_loss: 0.3962 - val_acc: 0.8101\n",
      "Epoch 18/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.3552 - acc: 0.8349Epoch 00017: val_acc did not improve\n",
      "327474/327474 [==============================] - 17s - loss: 0.3553 - acc: 0.8348 - val_loss: 0.3965 - val_acc: 0.8087\n",
      "Epoch 19/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.3529 - acc: 0.8358Epoch 00018: val_acc improved from 0.81007 to 0.81230, saving model to base 4.h5\n",
      "327474/327474 [==============================] - 18s - loss: 0.3528 - acc: 0.8358 - val_loss: 0.3951 - val_acc: 0.8123\n",
      "Epoch 20/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.3502 - acc: 0.8372Epoch 00019: val_acc did not improve\n",
      "327474/327474 [==============================] - 17s - loss: 0.3502 - acc: 0.8372 - val_loss: 0.3982 - val_acc: 0.8083\n",
      "Epoch 21/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.3480 - acc: 0.8381Epoch 00020: val_acc did not improve\n",
      "327474/327474 [==============================] - 17s - loss: 0.3480 - acc: 0.8381 - val_loss: 0.3967 - val_acc: 0.8081\n",
      "Epoch 22/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.3452 - acc: 0.8408Epoch 00021: val_acc did not improve\n",
      "327474/327474 [==============================] - 17s - loss: 0.3452 - acc: 0.8408 - val_loss: 0.3964 - val_acc: 0.8085\n",
      "Epoch 23/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.3433 - acc: 0.8412Epoch 00022: val_acc did not improve\n",
      "327474/327474 [==============================] - 19s - loss: 0.3433 - acc: 0.8412 - val_loss: 0.3980 - val_acc: 0.8085\n",
      "Epoch 24/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.3415 - acc: 0.8427Epoch 00023: val_acc did not improve\n",
      "327474/327474 [==============================] - 18s - loss: 0.3415 - acc: 0.8427 - val_loss: 0.3932 - val_acc: 0.8117\n",
      "Epoch 25/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.3400 - acc: 0.8433- ETA: 1s - loss: Epoch 00024: val_acc did not improve\n",
      "327474/327474 [==============================] - 19s - loss: 0.3401 - acc: 0.8432 - val_loss: 0.3981 - val_acc: 0.8092\n",
      "Epoch 26/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.3378 - acc: 0.8449- ETA: 0s - loss: 0.3374 - acc: Epoch 00025: val_acc did not improve\n",
      "327474/327474 [==============================] - 17s - loss: 0.3378 - acc: 0.8448 - val_loss: 0.3957 - val_acc: 0.8094\n",
      "Epoch 27/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.3362 - acc: 0.8459Epoch 00026: val_acc did not improve\n",
      "327474/327474 [==============================] - 20s - loss: 0.3362 - acc: 0.8459 - val_loss: 0.3993 - val_acc: 0.8103\n",
      "Epoch 28/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.3347 - acc: 0.8455Epoch 00027: val_acc improved from 0.81230 to 0.81419, saving model to base 4.h5\n",
      "327474/327474 [==============================] - 19s - loss: 0.3347 - acc: 0.8455 - val_loss: 0.3912 - val_acc: 0.8142\n",
      "Epoch 29/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.3333 - acc: 0.8474Epoch 00028: val_acc did not improve\n",
      "327474/327474 [==============================] - 18s - loss: 0.3332 - acc: 0.8474 - val_loss: 0.3987 - val_acc: 0.8096\n",
      "Epoch 30/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.3320 - acc: 0.8481Epoch 00029: val_acc improved from 0.81419 to 0.81526, saving model to base 4.h5\n",
      "327474/327474 [==============================] - 19s - loss: 0.3320 - acc: 0.8481 - val_loss: 0.3911 - val_acc: 0.8153\n",
      "Epoch 31/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.3306 - acc: 0.8486Epoch 00030: val_acc did not improve\n",
      "327474/327474 [==============================] - 18s - loss: 0.3307 - acc: 0.8486 - val_loss: 0.3924 - val_acc: 0.8128\n",
      "Epoch 32/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.3286 - acc: 0.8495Epoch 00031: val_acc did not improve\n",
      "327474/327474 [==============================] - 17s - loss: 0.3285 - acc: 0.8495 - val_loss: 0.3948 - val_acc: 0.8107\n",
      "Epoch 33/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.3276 - acc: 0.8504Epoch 00032: val_acc did not improve\n",
      "327474/327474 [==============================] - 16s - loss: 0.3277 - acc: 0.8504 - val_loss: 0.3994 - val_acc: 0.8087\n",
      "Epoch 34/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.3266 - acc: 0.8507Epoch 00033: val_acc did not improve\n",
      "327474/327474 [==============================] - 16s - loss: 0.3266 - acc: 0.8507 - val_loss: 0.3950 - val_acc: 0.8113\n",
      "Epoch 35/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.3250 - acc: 0.8514Epoch 00034: val_acc did not improve\n",
      "327474/327474 [==============================] - 16s - loss: 0.3250 - acc: 0.8514 - val_loss: 0.3992 - val_acc: 0.8071\n",
      "Epoch 36/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.3245 - acc: 0.8520Epoch 00035: val_acc did not improve\n",
      "327474/327474 [==============================] - 19s - loss: 0.3244 - acc: 0.8520 - val_loss: 0.3915 - val_acc: 0.8136\n",
      "Epoch 37/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.3228 - acc: 0.8522Epoch 00036: val_acc did not improve\n",
      "327474/327474 [==============================] - 16s - loss: 0.3227 - acc: 0.8522 - val_loss: 0.3939 - val_acc: 0.8130\n",
      "Epoch 38/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.3213 - acc: 0.8538Epoch 00037: val_acc did not improve\n",
      "327474/327474 [==============================] - 16s - loss: 0.3213 - acc: 0.8538 - val_loss: 0.3972 - val_acc: 0.8126\n",
      "Epoch 39/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.3215 - acc: 0.8534Epoch 00038: val_acc did not improve\n",
      "327474/327474 [==============================] - 18s - loss: 0.3215 - acc: 0.8533 - val_loss: 0.3974 - val_acc: 0.8116\n",
      "Epoch 40/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.3202 - acc: 0.8541Epoch 00039: val_acc did not improve\n",
      "327474/327474 [==============================] - 16s - loss: 0.3203 - acc: 0.8541 - val_loss: 0.3991 - val_acc: 0.8116\n",
      "Epoch 41/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.3180 - acc: 0.8554Epoch 00040: val_acc did not improve\n",
      "327474/327474 [==============================] - 18s - loss: 0.3180 - acc: 0.8554 - val_loss: 0.3953 - val_acc: 0.8120\n",
      "Epoch 42/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.3176 - acc: 0.8556Epoch 00041: val_acc did not improve\n",
      "327474/327474 [==============================] - 16s - loss: 0.3176 - acc: 0.8556 - val_loss: 0.3925 - val_acc: 0.8142\n",
      "Epoch 43/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.3157 - acc: 0.8567Epoch 00042: val_acc did not improve\n",
      "327474/327474 [==============================] - 16s - loss: 0.3157 - acc: 0.8567 - val_loss: 0.3976 - val_acc: 0.8127\n",
      "Epoch 44/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.3158 - acc: 0.8564Epoch 00043: val_acc did not improve\n",
      "327474/327474 [==============================] - 15s - loss: 0.3157 - acc: 0.8564 - val_loss: 0.3975 - val_acc: 0.8147\n",
      "Epoch 45/100\n",
      "326000/327474 [============================>.] - ETA: 0s - loss: 0.3150 - acc: 0.8566Epoch 00044: val_acc did not improve\n",
      "327474/327474 [==============================] - 19s - loss: 0.3151 - acc: 0.8566 - val_loss: 0.4006 - val_acc: 0.8092\n",
      "Epoch 46/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.3145 - acc: 0.8574Epoch 00045: val_acc did not improve\n",
      "327474/327474 [==============================] - 16s - loss: 0.3144 - acc: 0.8574 - val_loss: 0.3972 - val_acc: 0.8128\n",
      "Epoch 47/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.3129 - acc: 0.8580Epoch 00046: val_acc improved from 0.81526 to 0.81565, saving model to base 4.h5\n",
      "327474/327474 [==============================] - 20s - loss: 0.3129 - acc: 0.8580 - val_loss: 0.3956 - val_acc: 0.8156\n",
      "Epoch 48/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.3121 - acc: 0.8588Epoch 00047: val_acc did not improve\n",
      "327474/327474 [==============================] - 16s - loss: 0.3123 - acc: 0.8587 - val_loss: 0.3949 - val_acc: 0.8122\n",
      "Epoch 49/100\n",
      "326500/327474 [============================>.] - ETA: 0s - loss: 0.3111 - acc: 0.8597Epoch 00048: val_acc did not improve\n",
      "327474/327474 [==============================] - 18s - loss: 0.3111 - acc: 0.8596 - val_loss: 0.3959 - val_acc: 0.8117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efe5f4ff290>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x1_train, x2_train], y=y_train, batch_size=500, epochs=100,\n",
    "verbose=1, validation_split=0.1, shuffle=True, callbacks=[checkpoint,estop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#81.5% accuracy with a CNN model which is not a bad score.\n",
    "K.clear_session()\n",
    "del q1\n",
    "del q2\n",
    "del merged\n",
    "del model\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base 5\n",
      "Build Model . .\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_4 (InputLayer)             (None, 30)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)          (None, 30, 300)       26510400    input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)          (None, 30, 300)       26510400    input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                    (None, 30, 30)        39720       embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                    (None, 30, 30)        39720       embedding_4[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "add_1 (Add)                      (None, 30, 30)        0           lstm_3[0][0]                     \n",
      "                                                                   lstm_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 900)           0           add_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 900)           3600        flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 90)            81090       batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 90)            0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 90)            360         dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 30)            2730        batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 30)            0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 30)            120         dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1)             31          batch_normalization_3[0][0]      \n",
      "====================================================================================================\n",
      "Total params: 53,188,171\n",
      "Trainable params: 165,331\n",
      "Non-trainable params: 53,022,840\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Base 5')\n",
    "print('Build Model . .')\n",
    "\n",
    "question1 = Input(shape=(30,))\n",
    "question2 = Input(shape=(30,))\n",
    "\n",
    "q1 = Embedding(len(word_index) + 1, \n",
    "                 300, \n",
    "                 weights=[embedding_matrix], \n",
    "                 input_length=30, \n",
    "                 trainable=False)(question1)\n",
    "q1 = LSTM(30, return_sequences=True)(q1)\n",
    "\n",
    "q2 = Embedding(len(word_index) + 1, \n",
    "                 300, \n",
    "                 weights=[embedding_matrix], \n",
    "                 input_length=30, \n",
    "                 trainable=False)(question2)\n",
    "q2 = LSTM(30, return_sequences=True)(q2)\n",
    "\n",
    "\n",
    "merged = add([q1,q2])\n",
    "merged = Flatten()(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(90, activation='relu')(merged)\n",
    "merged = Dropout(0.1)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(30, activation='relu')(merged)\n",
    "merged = Dropout(0.1)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model(inputs=[question1,question2], outputs=merged)\n",
    "\n",
    "checkpoint = ModelCheckpoint('base 5.h5', monitor='val_acc', save_best_only=True, verbose=2)\n",
    "\n",
    "estop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=20, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 327474 samples, validate on 36387 samples\n",
      "Epoch 1/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.5667 - acc: 0.7009Epoch 00000: val_acc improved from -inf to 0.73694, saving model to base 5.h5\n",
      "327474/327474 [==============================] - 40s - loss: 0.5666 - acc: 0.7009 - val_loss: 0.5244 - val_acc: 0.7369\n",
      "Epoch 2/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.4899 - acc: 0.7573Epoch 00001: val_acc improved from 0.73694 to 0.75653, saving model to base 5.h5\n",
      "327474/327474 [==============================] - 45s - loss: 0.4898 - acc: 0.7573 - val_loss: 0.4826 - val_acc: 0.7565\n",
      "Epoch 3/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.4532 - acc: 0.7791Epoch 00002: val_acc improved from 0.75653 to 0.77739, saving model to base 5.h5\n",
      "327474/327474 [==============================] - 48s - loss: 0.4532 - acc: 0.7791 - val_loss: 0.4549 - val_acc: 0.7774\n",
      "Epoch 4/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.4267 - acc: 0.7950Epoch 00003: val_acc improved from 0.77739 to 0.78506, saving model to base 5.h5\n",
      "327474/327474 [==============================] - 49s - loss: 0.4267 - acc: 0.7950 - val_loss: 0.4431 - val_acc: 0.7851\n",
      "Epoch 5/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.4048 - acc: 0.8080Epoch 00004: val_acc improved from 0.78506 to 0.78940, saving model to base 5.h5\n",
      "327474/327474 [==============================] - 50s - loss: 0.4048 - acc: 0.8080 - val_loss: 0.4360 - val_acc: 0.7894\n",
      "Epoch 6/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.3875 - acc: 0.8183Epoch 00005: val_acc improved from 0.78940 to 0.79111, saving model to base 5.h5\n",
      "327474/327474 [==============================] - 50s - loss: 0.3875 - acc: 0.8182 - val_loss: 0.4386 - val_acc: 0.7911\n",
      "Epoch 7/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.3706 - acc: 0.8279Epoch 00006: val_acc improved from 0.79111 to 0.79418, saving model to base 5.h5\n",
      "327474/327474 [==============================] - 54s - loss: 0.3705 - acc: 0.8279 - val_loss: 0.4300 - val_acc: 0.7942\n",
      "Epoch 8/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.3570 - acc: 0.8361Epoch 00007: val_acc improved from 0.79418 to 0.79611, saving model to base 5.h5\n",
      "327474/327474 [==============================] - 52s - loss: 0.3570 - acc: 0.8361 - val_loss: 0.4411 - val_acc: 0.7961\n",
      "Epoch 9/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.3438 - acc: 0.8424Epoch 00008: val_acc improved from 0.79611 to 0.79899, saving model to base 5.h5\n",
      "327474/327474 [==============================] - 51s - loss: 0.3438 - acc: 0.8424 - val_loss: 0.4294 - val_acc: 0.7990\n",
      "Epoch 10/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.3320 - acc: 0.8492Epoch 00009: val_acc improved from 0.79899 to 0.80078, saving model to base 5.h5\n",
      "327474/327474 [==============================] - 49s - loss: 0.3319 - acc: 0.8492 - val_loss: 0.4339 - val_acc: 0.8008\n",
      "Epoch 11/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.3206 - acc: 0.8553Epoch 00010: val_acc did not improve\n",
      "327474/327474 [==============================] - 50s - loss: 0.3205 - acc: 0.8553 - val_loss: 0.4416 - val_acc: 0.7980\n",
      "Epoch 12/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.3105 - acc: 0.8601Epoch 00011: val_acc did not improve\n",
      "327474/327474 [==============================] - 50s - loss: 0.3105 - acc: 0.8601 - val_loss: 0.4518 - val_acc: 0.7969\n",
      "Epoch 13/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.3020 - acc: 0.8651Epoch 00012: val_acc did not improve\n",
      "327474/327474 [==============================] - 50s - loss: 0.3021 - acc: 0.8651 - val_loss: 0.4554 - val_acc: 0.7989\n",
      "Epoch 14/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2929 - acc: 0.8699Epoch 00013: val_acc did not improve\n",
      "327474/327474 [==============================] - 48s - loss: 0.2929 - acc: 0.8699 - val_loss: 0.4632 - val_acc: 0.7994\n",
      "Epoch 15/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2844 - acc: 0.8739Epoch 00014: val_acc did not improve\n",
      "327474/327474 [==============================] - 47s - loss: 0.2843 - acc: 0.8740 - val_loss: 0.4631 - val_acc: 0.7988\n",
      "Epoch 16/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2779 - acc: 0.8772Epoch 00015: val_acc did not improve\n",
      "327474/327474 [==============================] - 50s - loss: 0.2779 - acc: 0.8772 - val_loss: 0.4807 - val_acc: 0.7992\n",
      "Epoch 17/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2700 - acc: 0.8807Epoch 00016: val_acc did not improve\n",
      "327474/327474 [==============================] - 49s - loss: 0.2700 - acc: 0.8807 - val_loss: 0.4864 - val_acc: 0.7997\n",
      "Epoch 18/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2633 - acc: 0.8847Epoch 00017: val_acc did not improve\n",
      "327474/327474 [==============================] - 52s - loss: 0.2633 - acc: 0.8847 - val_loss: 0.4842 - val_acc: 0.8007\n",
      "Epoch 19/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2583 - acc: 0.8868Epoch 00018: val_acc did not improve\n",
      "327474/327474 [==============================] - 56s - loss: 0.2584 - acc: 0.8868 - val_loss: 0.4962 - val_acc: 0.7998\n",
      "Epoch 20/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2521 - acc: 0.8904Epoch 00019: val_acc did not improve\n",
      "327474/327474 [==============================] - 54s - loss: 0.2521 - acc: 0.8904 - val_loss: 0.5128 - val_acc: 0.7934\n",
      "Epoch 21/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2462 - acc: 0.8934Epoch 00020: val_acc did not improve\n",
      "327474/327474 [==============================] - 55s - loss: 0.2462 - acc: 0.8933 - val_loss: 0.5211 - val_acc: 0.7942\n",
      "Epoch 22/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2421 - acc: 0.8947Epoch 00021: val_acc did not improve\n",
      "327474/327474 [==============================] - 52s - loss: 0.2420 - acc: 0.8947 - val_loss: 0.5197 - val_acc: 0.7964\n",
      "Epoch 23/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2363 - acc: 0.8977Epoch 00022: val_acc did not improve\n",
      "327474/327474 [==============================] - 54s - loss: 0.2363 - acc: 0.8977 - val_loss: 0.5357 - val_acc: 0.7984\n",
      "Epoch 24/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2321 - acc: 0.8999Epoch 00023: val_acc did not improve\n",
      "327474/327474 [==============================] - 52s - loss: 0.2321 - acc: 0.8999 - val_loss: 0.5354 - val_acc: 0.7989\n",
      "Epoch 25/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2280 - acc: 0.9024- ETA: 1s - loss: 0.2276 - aEpoch 00024: val_acc did not improve\n",
      "327474/327474 [==============================] - 55s - loss: 0.2281 - acc: 0.9024 - val_loss: 0.5566 - val_acc: 0.7976\n",
      "Epoch 26/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2245 - acc: 0.9035Epoch 00025: val_acc did not improve\n",
      "327474/327474 [==============================] - 60s - loss: 0.2245 - acc: 0.9035 - val_loss: 0.5511 - val_acc: 0.7962\n",
      "Epoch 27/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2194 - acc: 0.9062Epoch 00026: val_acc did not improve\n",
      "327474/327474 [==============================] - 54s - loss: 0.2195 - acc: 0.9062 - val_loss: 0.5716 - val_acc: 0.7969\n",
      "Epoch 28/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2161 - acc: 0.9081Epoch 00027: val_acc did not improve\n",
      "327474/327474 [==============================] - 54s - loss: 0.2161 - acc: 0.9082 - val_loss: 0.5599 - val_acc: 0.7948\n",
      "Epoch 29/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2131 - acc: 0.9094Epoch 00028: val_acc did not improve\n",
      "327474/327474 [==============================] - 51s - loss: 0.2131 - acc: 0.9094 - val_loss: 0.5825 - val_acc: 0.7985\n",
      "Epoch 30/100\n",
      "327000/327474 [============================>.] - ETA: 0s - loss: 0.2096 - acc: 0.9108Epoch 00029: val_acc did not improve\n",
      "327474/327474 [==============================] - 55s - loss: 0.2096 - acc: 0.9108 - val_loss: 0.5589 - val_acc: 0.7923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fece1027290>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x1_train, x2_train], y=y_train, batch_size=1000, epochs=100,\n",
    "verbose=1, validation_split=0.1, shuffle=True, callbacks=[checkpoint,estop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
